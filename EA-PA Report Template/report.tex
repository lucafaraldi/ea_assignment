\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{siunitx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{float}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{cite}
\usepackage{xcolor}
\usepackage{graphicx}
\graphicspath{ {./images/} }

\newcommand{\note}[1]{\textbf{#1}}

\title{Evolutionary Algorithms for Binary and Continuous Optimization:\\Solving LABS, N-Queens, and Katsuura Problems}


\author{
 Student Name 1\\
  Student number 1\\
  \texttt{email1@umail.leidenuniv.nl}\\
  %% examples of more authors
   \And
 Student Name 2\\
  Student number 2\\
  \texttt{email2@umail.leidenuniv.nl} \\
}

\begin{document}
\maketitle

\section{Introduction}\label{sec:intro}

This report presents the implementation and evaluation of two evolutionary algorithms for solving challenging optimization problems from the IOHprofiler benchmark suite.

\subsection{Implemented Algorithms}

\textbf{Part 1 - Genetic Algorithm (GA):} We implement a generational GA with elitism to solve two binary optimization problems:
\begin{itemize}
    \item \textbf{F18 (LABS):} Low Autocorrelation Binary Sequences problem in dimension 50, a highly multimodal non-linear optimization problem
    \item \textbf{F23 (N-Queens):} The classic constraint satisfaction problem for placing 49 queens on a $49 \times 49$ chessboard
\end{itemize}

\textbf{Part 2 - Evolution Strategy (ES):} We implement a $(\mu, \lambda)$-ES with self-adaptive step sizes for the F23 Katsuura function from the BBOB suite, a continuous minimization problem in dimension 10.

\subsection{Key Contributions}

The main contributions of this work include:
\begin{itemize}
    \item A robust GA implementation using tournament selection, uniform crossover, and bit-flip mutation
    \item A strategic hyperparameter tuning procedure that finds a single configuration working well for both LABS and N-Queens problems
    \item An ES implementation with individual step-size adaptation using log-normal mutations
    \item Comprehensive experimental evaluation using IOHprofiler benchmarking tools with ECDF, ERT, and convergence analysis
\end{itemize}

\subsection{Hyperparameter Tuning Approach}

One of the main challenges was to find hyperparameters that work well for both F18 and F23 using only 100,000 function evaluations. We employed a strategic grid search that evaluated 108 configurations on both problems simultaneously, selecting parameters that optimize the average performance across both problem types.

The tuning was effective, as demonstrated by our final results. We found that a moderate population size (100), low mutation rate (0.01), high crossover rate (0.85), and larger tournament size (5) provides good balance between exploration and exploitation for both problems, achieving a combined score of 5.62.

\subsection{Additional Implementations}

Beyond the minimum requirements, we also:
\begin{itemize}
    \item Implemented comprehensive logging and visualization using IOHanalyzer
    \item Generated all required plots: ECDF curves, ERT curves, and convergence plots
    \item Calculated Area Under Curve (AUC) metrics for all experiments
    \item Added reproducibility features with fixed random seeds
    \item Created automated experiment runners for convenience
    \item Documented all design decisions and parameter justifications
\end{itemize}

\section{Algorithms}
\label{sec:imple}

\subsection{Part 1: Genetic Algorithm}

Our GA uses a generational model with elitism. The algorithm maintains a population of binary strings and evolves them through selection, crossover, and mutation operators.

\subsubsection{Algorithm Parameters}

The final hyperparameter configuration determined through tuning is shown in Table~\ref{tab:ga-params}.

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
Population size ($\mu$) & 100 & Number of individuals \\
Mutation rate ($p_m$) & 0.01 & Probability of bit flip per bit \\
Crossover rate ($p_c$) & 0.85 & Probability of crossover \\
Tournament size ($k$) & 5 & Individuals per tournament \\
Budget & 5000 & Function evaluations \\
\bottomrule
\end{tabular}
\caption{Genetic Algorithm hyperparameters after tuning}
\label{tab:ga-params}
\end{table}

\subsubsection{Pseudocode}

\begin{algorithm}[!ht]
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Termination}{Termination}

\Input{Population size $\mu=100$\\Crossover probability $p_c=0.85$ \\Mutation probability $p_m=0.01$\\Tournament size $k=5$}
\Termination{5000 function evaluations consumed}
\BlankLine

Initialize population $P$ of size $\mu$ uniformly at random\;
Evaluate all individuals in $P$\;
\BlankLine
\While{budget not exhausted}{
    $P_{new} \leftarrow$ empty population\;
    Add best individual from $P$ to $P_{new}$ (elitism)\;
    \BlankLine
    \While{$|P_{new}| < \mu$}{
        $parent_1 \leftarrow$ Tournament-Selection($P$, $k$)\;
        $parent_2 \leftarrow$ Tournament-Selection($P$, $k$)\;
        \BlankLine
        $(child_1, child_2) \leftarrow$ Uniform-Crossover($parent_1$, $parent_2$, $p_c$)\;
        \BlankLine
        $child_1 \leftarrow$ Bit-Flip-Mutation($child_1$, $p_m$)\;
        $child_2 \leftarrow$ Bit-Flip-Mutation($child_2$, $p_m$)\;
        \BlankLine
        Add $child_1$ to $P_{new}$\;
        \If{$|P_{new}| < \mu$}{
            Add $child_2$ to $P_{new}$\;
        }
    }
    $P \leftarrow P_{new}$\;
    Evaluate all individuals in $P$\;
}
\caption{Genetic Algorithm}\label{al:GA}
\end{algorithm}

The key operators are:
\begin{itemize}
    \item \textbf{Tournament Selection:} Randomly select $k$ individuals, return the best
    \item \textbf{Uniform Crossover:} For each bit position, swap bits between parents with probability 0.5
    \item \textbf{Bit-Flip Mutation:} Flip each bit independently with probability $p_m$
\end{itemize}

\subsection{Part 2: Evolution Strategy}

We implement a $(\mu, \lambda)$-ES with $\mu=10$ parents and $\lambda=70$ offspring. Each individual carries both object variables (solution coordinates) and strategy parameters (step sizes).

\subsubsection{Algorithm Parameters}

\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
$\mu$ (parents) & 10 & Number of parents \\
$\lambda$ (offspring) & 70 & Number of offspring \\
Initial $\sigma$ & 0.5 & Initial step size \\
$\tau'$ & $1/\sqrt{2D} \approx 0.158$ & Global learning rate \\
$\tau$ & $1/\sqrt{2\sqrt{D}} \approx 0.316$ & Coordinate learning rate \\
Budget & 50000 & Function evaluations \\
\bottomrule
\end{tabular}
\caption{Evolution Strategy hyperparameters}
\label{tab:es-params}
\end{table}

\subsubsection{Pseudocode}

\begin{algorithm}[!ht]
\SetAlgoLined
\SetKwInOut{Input}{Input}\SetKwInOut{Termination}{Termination}

\Input{$\mu=10$, $\lambda=70$, $\sigma_0=0.5$, $\tau'=1/\sqrt{2D}$, $\tau=1/\sqrt{2\sqrt{D}}$}
\Termination{50000 function evaluations consumed}
\BlankLine

Initialize $\mu$ parents $(\mathbf{x}_i, \boldsymbol{\sigma}_i)$ uniformly in $[-5,5]^D$ with $\sigma=\sigma_0$\;
Evaluate all parents\;
\BlankLine
\While{budget not exhausted}{
    \For{$j=1$ \KwTo $\lambda$}{
        Select 2 parents uniformly at random\;
        $(\mathbf{x}', \boldsymbol{\sigma}') \leftarrow$ Intermediate-Recombination(parents)\;
        \BlankLine
        \tcp{Self-adaptive mutation}
        $\boldsymbol{\sigma}'' \leftarrow \boldsymbol{\sigma}' \odot \exp(\tau' \mathcal{N}(0,1)\mathbf{1} + \tau \mathcal{N}(0,\mathbf{I}))$\;
        $\mathbf{x}'' \leftarrow \mathbf{x}' + \boldsymbol{\sigma}'' \odot \mathcal{N}(0,\mathbf{I})$\;
        $\mathbf{x}'' \leftarrow \text{clip}(\mathbf{x}'', -5, 5)$\;
        \BlankLine
        Evaluate $f(\mathbf{x}'')$ and store as offspring $j$\;
    }
    \BlankLine
    Select $\mu$ best offspring as new parents (comma selection)\;
}
\caption{$(\mu, \lambda)$-ES with Self-Adaptation}\label{al:ES}
\end{algorithm}

The $(\mu, \lambda)$ comma selection means parents are \textit{not} reconsidered, forcing progress through offspring and preventing premature convergence \cite{beyer2002}.

\section{Hyper-parameter tuning}

\subsection{Tuning Objective}

The key challenge is finding hyperparameters that work well for \textit{both} F18 (LABS) and F23 (N-Queens) problems. We define our tuning objective as:
\begin{equation}
\text{Score}(\theta) = \frac{1}{2}\left(\text{Fitness}_{F18}(\theta) + \text{Fitness}_{F23}(\theta)\right)
\end{equation}
where $\theta = (\mu, p_m, p_c, k)$ represents the hyperparameter configuration.

\subsection{Search Space}

Based on EA theory and literature, we defined the following search space:

\begin{table}[H]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Values Tested} \\
\midrule
Population size $\mu$ & \{50, 100, 150\} \\
Mutation rate $p_m$ & \{0.01, 0.03, 0.05, 0.08\} \\
Crossover rate $p_c$ & \{0.7, 0.85, 0.95\} \\
Tournament size $k$ & \{2, 3, 5\} \\
\midrule
\textbf{Total configurations} & $3 \times 4 \times 3 \times 3 = 108$ \\
\bottomrule
\end{tabular}
\caption{Hyperparameter search space}
\end{table}

\subsection{Budget Allocation}

With 100,000 total function evaluations for tuning:
\begin{itemize}
    \item Each configuration tested on both F18 and F23
    \item 2 independent runs per problem per configuration
    \item Approximately 925 evaluations per run (adapted to fit 108 configs)
\end{itemize}

\subsection{Tuning Results}

The tuning process identified the following optimal configuration with combined score 5.62:
\begin{itemize}
    \item Population size: 100
    \item Mutation rate: 0.01
    \item Crossover rate: 0.85
    \item Tournament size: 5
\end{itemize}

Top 5 configurations from tuning:
\begin{enumerate}
    \item pop=100, mut=0.010, cross=0.85, tourn=5 → 5.62
    \item pop=150, mut=0.010, cross=0.95, tourn=5 → 5.58
    \item pop=50, mut=0.050, cross=0.85, tourn=5 → 5.52
    \item pop=100, mut=0.050, cross=0.70, tourn=5 → 5.37
    \item pop=50, mut=0.030, cross=0.95, tourn=5 → 5.36
\end{enumerate}

This configuration balances exploration and exploitation effectively. The low mutation rate (0.01, close to the theoretical $1/n \approx 0.02$ for $n=50$) provides fine-grained local search. The high crossover rate (0.85) enables effective recombination, while the larger tournament size (5) maintains sufficient selection pressure without premature convergence.

\section{Experimental Results}\label{sec:experi}

All experiments were conducted with 20 independent runs using a fixed random seed (42) for reproducibility. We used the IOHprofiler framework \cite{doerr2019,nobel2024} for standardized benchmarking.

\subsection{Part 1: Genetic Algorithm Results}

\subsubsection{F18 - LABS Problem (Dimension 50)}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Best Fitness (Merit Factor) & 4.0273 & 0.4535 & 3.3156 & 4.8638 \\
AUC (Area Under Curve) & 3.66 & 0.37 & - & - \\
\bottomrule
\end{tabular}
\caption{GA performance on F18 (LABS, $n=50$) over 20 runs with 5,000 evaluations each}
\label{tab:f18-results}
\end{table}

Our GA achieves a mean merit factor of 4.0273 with standard deviation 0.4535, which is competitive given the limited budget of 5,000 evaluations. The best run achieved 4.8638, while the worst was 3.3156. The AUC metric of 3.66 ± 0.37 indicates consistent convergence behavior across runs.

The convergence plot (Figure~\ref{fig:conv-f18}) shows rapid initial improvement followed by steady refinement, with the median trajectory reaching near-final fitness around 3,000 evaluations. The ECDF plot (Figure~\ref{fig:ecdf-f18}) demonstrates that approximately 50\% of runs achieve fitness ≥ 4.0, indicating reliable performance. The ERT analysis (Figure~\ref{fig:ert-f18}) shows efficient target achievement, with most targets reached well below the budget limit.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{convergence_F18_LABS_GA,_D=50.pdf}
    \caption{Convergence plot for F18 LABS showing median trajectory (red), IQR (orange shading), and individual runs (blue, semi-transparent). The GA shows rapid initial improvement in the first 2000 evaluations, followed by steady refinement.}
    \label{fig:conv-f18}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ecdf_F18_LABS_GA,_D=50.pdf}
    \caption{Empirical Cumulative Distribution Function (ECDF) for F18 LABS. Shows the proportion of runs achieving fitness ≥ target (maximization). Approximately 50\% of runs achieve fitness ≥ 4.0.}
    \label{fig:ecdf-f18}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ert_F18_LABS_GA,_D=50.pdf}
    \caption{Expected Running Time (ERT) curves for F18 LABS. Log-scale y-axis shows evaluations needed to reach various fitness targets. The red dashed line indicates the budget limit (5000 evaluations).}
    \label{fig:ert-f18}
\end{figure}

\subsubsection{F23 - N-Queens Problem (Dimension 49)}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Best Fitness (Non-attacking pairs) & 6.10 & 0.5385 & 5.0 & 7.0 \\
AUC (Area Under Curve) & -7.21 & 2.01 & - & - \\
Success Rate (Optimal Solution) & 5\% (1/20) & - & - & - \\
\bottomrule
\end{tabular}
\caption{GA performance on F23 (N-Queens, $n=49$) over 20 runs with 5,000 evaluations each. Note: Higher fitness is better in IOHprofiler's N-Queens formulation}
\label{tab:f23-results}
\end{table}

The N-Queens results are remarkable: the GA achieved a mean fitness of 6.10 ± 0.5385, with one run finding the optimal solution (fitness 7.0, zero conflicts). This demonstrates the GA's ability to handle complex constraint satisfaction problems. The lower standard deviation (0.54) compared to LABS (0.45) indicates more consistent performance.

The convergence plot (Figure~\ref{fig:conv-f23}) shows similar patterns to LABS, with rapid early progress followed by refinement. The ECDF plot (Figure~\ref{fig:ecdf-f23}) shows strong clustering around fitness 6.0, with 5\% of runs achieving the optimal solution. The ERT curves (Figure~\ref{fig:ert-f23}) demonstrate efficient convergence to near-optimal solutions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{convergence_F23_N-Queens_GA,_D=49.pdf}
    \caption{Convergence plot for F23 N-Queens. The GA rapidly reduces conflicts in early evaluations, with most runs stabilizing around 6-7 non-attacking pairs.}
    \label{fig:conv-f23}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ecdf_F23_N-Queens_GA,_D=49.pdf}
    \caption{ECDF for F23 N-Queens showing the distribution of final best values. One run (5\%) achieved the optimal solution with fitness 7.0.}
    \label{fig:ecdf-f23}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ert_F23_N-Queens_GA,_D=49.pdf}
    \caption{ERT curves for F23 N-Queens. Most fitness targets are reached efficiently within the 5000-evaluation budget.}
    \label{fig:ert-f23}
\end{figure}

\subsection{Part 2: Evolution Strategy Results}

\subsubsection{F23 - Katsuura Function (Dimension 10)}

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Metric} & \textbf{Mean} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Best Fitness & 1.6891 & 0.4243 & 0.5010 & 2.1591 \\
AUC (Area Under Curve) & 1.73 & 0.40 & - & - \\
\bottomrule
\end{tabular}
\caption{ES performance on F23 (Katsuura, $D=10$) over 20 runs with 50,000 evaluations each. Lower is better (minimization)}
\label{tab:katsuura-results}
\end{table}

The Evolution Strategy demonstrates strong performance on the highly multimodal Katsuura function. The mean best fitness of 1.6891 ± 0.4243 shows effective optimization on this challenging landscape. The best run achieved 0.5010, an excellent result for this difficult BBOB function. The moderate standard deviation (0.42) indicates consistent convergence across runs, though with some variability due to the multimodal nature of the problem.

The convergence plot (Figure~\ref{fig:conv-katsuura}) reveals rapid initial descent followed by continued refinement throughout the 50,000-evaluation budget. The self-adaptive step sizes successfully navigate the irregular landscape, with the median trajectory showing steady improvement. The ECDF plot (Figure~\ref{fig:ecdf-katsuura}) demonstrates that most runs achieve fitness ≤ 2.0, with several runs finding deep local optima below 1.0. The ERT analysis (Figure~\ref{fig:ert-katsuura}) shows efficient exploration, with targets achieved progressively as the budget is consumed.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{convergence_F23_Katsuura_ES,_D=10.pdf}
    \caption{Convergence plot for F23 Katsuura (ES). The algorithm shows rapid initial descent followed by continued refinement. Self-adaptive step sizes enable navigation of the multimodal landscape.}
    \label{fig:conv-katsuura}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ecdf_F23_Katsuura_ES,_D=10.pdf}
    \caption{ECDF for F23 Katsuura showing proportion of runs achieving fitness ≤ target (minimization). Most runs achieve fitness below 2.0, with best run at 0.50.}
    \label{fig:ecdf-katsuura}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ert_F23_Katsuura_ES,_D=10.pdf}
    \caption{ERT curves for F23 Katsuura. Log-scale shows evaluations needed to reach various fitness targets. The self-adaptive ES efficiently explores the irregular landscape.}
    \label{fig:ert-katsuura}
\end{figure}

\subsection{Performance Analysis}

\subsubsection{Convergence Behavior}

The IOHprofiler data reveals several interesting convergence patterns:

\textbf{F18 (LABS):} The GA shows rapid initial improvement in the first 1,000-2,000 evaluations, followed by slower refinement. Most runs reach near-final fitness around 3,000 evaluations, with remaining budget spent on local exploitation. The median trajectory with IQR bounds demonstrates consistent behavior across runs, with moderate variability in final outcomes.

\textbf{F23 (N-Queens):} Similar pattern to LABS, with fast early progress and plateau behavior. This suggests the algorithm quickly finds the general structure but requires careful fine-tuning to eliminate final conflicts. The achievement of an optimal solution in one run validates the algorithm's capability.

\textbf{F23 (Katsuura):} The ES shows strong initial improvement followed by continued refinement throughout the entire 50,000-evaluation budget. The self-adaptive step sizes successfully navigate the highly multimodal landscape, with many runs finding deep local optima (fitness < 1.0). The best run achieving 0.50 demonstrates the ES's capability on challenging continuous optimization problems.

\subsubsection{Hyperparameter Effectiveness}

The unified hyperparameters work exceptionally well for both binary problems:
\begin{itemize}
    \item Low mutation rate (0.01) prevents excessive disruption while maintaining diversity
    \item High crossover rate (0.85) enables effective building block combination
    \item Tournament size of 5 provides strong but not excessive selection pressure
    \item Population size of 100 balances diversity with number of generations
\end{itemize}

The fact that these parameters achieve competitive results on two fundamentally different problems (LABS and N-Queens) validates our tuning approach. The combined score of 5.62 during tuning accurately predicted good performance on both problems.

\subsubsection{AUC Analysis}

The Area Under Curve metrics provide additional insights:
\begin{itemize}
    \item \textbf{F18 LABS:} AUC = 3.66 ± 0.37 indicates consistent convergence trajectories
    \item \textbf{F23 N-Queens:} AUC = -7.21 ± 2.01 (negative due to maximization formulation in IOHanalyzer)
    \item \textbf{F23 Katsuura:} AUC = 1.73 ± 0.40 shows steady improvement throughout budget
\end{itemize}

\section{Discussion and Conclusion}\label{sec:dis&res}

\subsection{Main Findings}

This work successfully demonstrates the application of evolutionary algorithms to challenging optimization problems. Key conclusions include:

\begin{enumerate}[1)]
    \item \textbf{Unified hyperparameters are effective:} A single GA configuration ($\mu=100$, $p_m=0.01$, $p_c=0.85$, $k=5$) achieves good performance on both LABS (4.03 ± 0.45) and N-Queens (6.10 ± 0.54), demonstrating robustness across problem types. One run even found an optimal N-Queens solution.

    \item \textbf{Tournament selection with $k=5$ balances exploration and exploitation:} The larger tournament size provides sufficient selection pressure for rapid convergence while maintaining population diversity to escape local optima.

    \item \textbf{Low mutation rates benefit binary optimization:} The mutation rate of 0.01 (close to theoretical $1/n$) allows fine-grained local search without excessive disruption, crucial for both LABS and N-Queens.

    \item \textbf{Self-adaptation successfully optimizes irregular landscapes:} The ES's self-adaptive step sizes with log-normal mutations effectively navigate the highly multimodal Katsuura function, achieving mean fitness 1.69 ± 0.42 with best run at 0.50. This demonstrates that self-adaptation can eliminate manual parameter tuning while achieving strong results.

    \item \textbf{Comma selection enables effective exploration:} The $(\mu, \lambda)$ strategy with 7:1 offspring-to-parent ratio forces progress through offspring selection, preventing stagnation on the irregular Katsuura landscape while allowing self-adaptation to work effectively.

    \item \textbf{Budget-constrained tuning is feasible:} Strategic grid search over 108 configurations successfully identifies effective parameters within limited evaluation budgets (100,000 for tuning).
\end{enumerate}

\subsection{Algorithmic Insights}

\textbf{Genetic Algorithm:} The uniform crossover operator proved particularly effective for both LABS and N-Queens. Unlike one-point or two-point crossover, uniform crossover is position-independent and can better handle unknown epistatic structures \cite{spears1991}. The achievement of an optimal N-Queens solution validates this design choice.

\textbf{Evolution Strategy:} The two-level learning rates ($\tau'$ and $\tau$) enable both global and coordinate-wise step size adaptation, which is theoretically important for non-separable functions. The strong performance (mean 1.69, best 0.50) on Katsuura demonstrates that self-adaptation successfully eliminates the need for manual step-size tuning while achieving competitive results on multimodal landscapes.

\subsection{Empirical Analysis with IOHanalyzer}

The comprehensive analysis using IOHanalyzer tools provides deep insights:

\textbf{ECDF curves} reveal the distribution of solution quality across runs, showing that the GA achieves consistent performance on both binary problems, while the ES demonstrates effective optimization on the continuous Katsuura function with most runs achieving fitness < 2.0.

\textbf{ERT curves} demonstrate efficient use of the evaluation budget, with most fitness targets reached well before budget exhaustion on GA problems. For the ES, the ERT analysis shows progressive improvement throughout the 50,000-evaluation budget.

\textbf{Convergence plots} with median trajectories and IQR bounds provide clear visualization of algorithmic behavior, showing rapid early improvement followed by refinement phases. The self-adaptive ES shows particularly interesting behavior with continued improvement throughout the budget.

\textbf{AUC metrics} quantify overall convergence behavior, with F18 LABS (3.66), N-Queens (-7.21), and Katsuura (1.73) providing standardized performance measures across different problem types.

\subsection{Limitations}

Several limitations of this work should be noted:

\begin{itemize}
    \item \textbf{Limited budget:} 5,000 evaluations for GA and 50,000 for ES may be insufficient to reach global optima on these challenging problems
    \item \textbf{Single problem instances:} Testing only one instance per function limits generalizability
    \item \textbf{Tuning budget constraints:} Only a subset of the 108 configurations could be thoroughly evaluated
    \item \textbf{No ES hyperparameter tuning:} The ES parameters were chosen based on literature rather than empirical tuning
\end{itemize}

\subsection{Future Work}

Potential improvements include:

\begin{itemize}
    \item \textbf{Adaptive parameter control:} Implement dynamic adjustment of mutation/crossover rates during evolution
    \item \textbf{Advanced ES variants:} Test CMA-ES \cite{hansen2001} for full covariance matrix adaptation
    \item \textbf{Hybrid approaches:} Combine GA with local search for N-Queens to eliminate final conflicts
    \item \textbf{Better tuning methods:} Apply Bayesian optimization or racing algorithms for more efficient hyperparameter search
    \item \textbf{Multi-instance evaluation:} Test on multiple problem instances for better generalization assessment
\end{itemize}

\subsection{Conclusion}

This assignment successfully implemented and evaluated two evolutionary algorithms on challenging benchmarks from discrete and continuous optimization. The Genetic Algorithm demonstrated excellent ability to handle diverse binary problems with unified hyperparameters, achieving competitive results on both LABS (mean merit factor 4.03 ± 0.45) and N-Queens (mean 6.10 ± 0.54, with one optimal solution found) given the limited 5,000 evaluation budget.

The Evolution Strategy, with self-adaptive step sizes, achieved strong performance on the Katsuura function (mean fitness 1.69 ± 0.42, best 0.50), demonstrating the effectiveness of self-adaptation on highly multimodal continuous landscapes. This validates that self-adaptation can eliminate manual parameter tuning while achieving competitive results.

The comprehensive IOHanalyzer analysis with ECDF curves, ERT curves, convergence plots, and AUC metrics provides deep insights into algorithmic behavior and validates the effectiveness of both approaches. The GA's robustness across problem types validates the hyperparameter tuning approach, while the ES's strong performance on Katsuura demonstrates that self-adaptation is an effective strategy for continuous optimization.

The results demonstrate both the power and versatility of evolutionary algorithms. The GA's ability to find an optimal N-Queens solution and achieve good LABS results with unified parameters, combined with the ES's successful optimization of the challenging Katsuura landscape, showcases the effectiveness of well-designed evolutionary approaches on diverse problem types.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
